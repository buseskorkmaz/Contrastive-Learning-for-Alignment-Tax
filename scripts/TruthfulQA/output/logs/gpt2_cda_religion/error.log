/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 2!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 4!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 5!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 10!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 11!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 12!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 14!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 17!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 18!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 20!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 21!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 22!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 23!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 24!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 25!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 26!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 28!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 29!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 30!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 34!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 35!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 36!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 37!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 41!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 44!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 47!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 50!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 51!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 52!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 53!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 55!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 56!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 58!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 59!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 60!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 61!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 62!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 63!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 65!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 66!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 68!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 69!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 70!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 71!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 73!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 74!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 75!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 79!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 83!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 86!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 87!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 91!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 95!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 98!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 100!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 101!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 103!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 104!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 105!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 107!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 108!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 109!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 110!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 111!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 112!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 113!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 114!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 135!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 136!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 137!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 138!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 140!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 141!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 143!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 144!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 145!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 147!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 148!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 150!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 152!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 154!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 157!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 158!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 164!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 165!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 166!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 171!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 172!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 174!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 178!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 179!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 185!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 188!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 193!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 194!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 196!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 198!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 199!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 201!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 202!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 203!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 206!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 207!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 209!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 210!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 219!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 220!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 223!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 224!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 229!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 233!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 249!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 250!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 251!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 252!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 255!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 258!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 261!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 262!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 263!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 269!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 270!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 277!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 278!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 279!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 281!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 284!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 294!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 299!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 304!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 306!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 324!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 328!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 332!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 339!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 340!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 341!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 342!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 348!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 355!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 356!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 357!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 359!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 360!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 362!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 373!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 374!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 382!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 383!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 384!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 385!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 388!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 389!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 391!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 392!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 393!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 394!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 395!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 396!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 397!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 398!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 399!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 407!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 408!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 409!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 410!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 411!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 413!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 415!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 416!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 418!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 419!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 420!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 421!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 422!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 423!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 424!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 426!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 427!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 428!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 429!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 430!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 431!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 432!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 433!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 434!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 435!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 436!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 437!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 438!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 439!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 440!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 441!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 442!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 448!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 451!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 452!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 454!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 455!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 456!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 458!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 463!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 468!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 469!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 470!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 471!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 475!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 476!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 478!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 479!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 480!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 481!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 485!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 488!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 490!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 493!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 495!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 497!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 499!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 500!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 502!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 503!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 504!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 505!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 508!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 509!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 510!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 511!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 519!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 520!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 521!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 522!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 523!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 526!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 527!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 528!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 531!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 532!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 533!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 534!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 535!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 536!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 537!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 538!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 539!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 540!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 543!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 547!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 548!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 549!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 550!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 551!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 555!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 557!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 558!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 559!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 560!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 561!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 562!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 563!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 564!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 565!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 566!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 567!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 568!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 569!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 571!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 572!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 574!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 575!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 577!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 578!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 582!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 584!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 585!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 587!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 589!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 590!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 593!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 594!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 595!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 596!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 598!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 601!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 602!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 603!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 604!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 605!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 607!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 608!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 609!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 610!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 611!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 618!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 619!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 620!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 621!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 622!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 623!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 624!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 625!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 626!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 627!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 631!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 632!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 633!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 634!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 635!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 636!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 637!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 641!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 649!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 650!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 653!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 655!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 661!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 665!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 667!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 669!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 673!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 682!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 685!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 693!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 694!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 695!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 697!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 698!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 703!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 704!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 710!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 720!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 723!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 726!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 727!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 734!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 739!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 745!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 747!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 748!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 749!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 756!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 759!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 762!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 765!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 766!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 767!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 771!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 773!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 779!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 781!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 783!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 784!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 785!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 790!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 793!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 795!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 800!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 802!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 804!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 806!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 807!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 808!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 809!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 810!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 812!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 813!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 814!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 815!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 816!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 2!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 4!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 5!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 10!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 11!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 12!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 14!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 17!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 18!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 20!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 21!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 22!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 23!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 24!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 25!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 26!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 28!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 29!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 30!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 34!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 35!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 36!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 37!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 41!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 44!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 47!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 50!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 51!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 52!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 53!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 55!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 56!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 58!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 59!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 60!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 61!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 62!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 63!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 65!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 66!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 68!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 69!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 70!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 71!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 73!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 74!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 75!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 79!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 83!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 86!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 87!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 91!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 95!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 98!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 100!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 101!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 103!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 104!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 105!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 107!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 108!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 109!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 110!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 111!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 112!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 113!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 114!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 135!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 136!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 137!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 138!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 140!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 141!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 143!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 144!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 145!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 147!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 148!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 150!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 152!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 154!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 157!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 158!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 164!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 165!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 166!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 171!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 172!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 174!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 178!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 179!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 185!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 188!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 193!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 194!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 196!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 198!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 199!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 201!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 202!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 203!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 206!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 207!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 209!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 210!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 219!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 220!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 223!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 224!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 229!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 233!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 249!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 250!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 251!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 252!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 255!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 258!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 261!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 262!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 263!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 269!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 270!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 277!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 278!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 279!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 281!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 284!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 294!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 299!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 304!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 306!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 324!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 328!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 332!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 339!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 340!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 341!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 342!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 348!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 355!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 356!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 357!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 359!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 360!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 362!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 373!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 374!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 382!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 383!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 384!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 385!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 388!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 389!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 391!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 392!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 393!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 394!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 395!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 396!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 397!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 398!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 399!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 407!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 408!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 409!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 410!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 411!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 413!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 415!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 416!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 418!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 419!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 420!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 421!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 422!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 423!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 424!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 426!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 427!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 428!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 429!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 430!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 431!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 432!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 433!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 434!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 435!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 436!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 437!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 438!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 439!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 440!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 441!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 442!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 448!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 451!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 452!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 454!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 455!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 456!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 458!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 463!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 468!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 469!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 470!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 471!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 475!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 476!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 478!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 479!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 480!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 481!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 485!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 488!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 490!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 493!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 495!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 497!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 499!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 500!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 502!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 503!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 504!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 505!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 508!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 509!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 510!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 511!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 519!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 520!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 521!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 522!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 523!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 526!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 527!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 528!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 531!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 532!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 533!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 534!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 535!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 536!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 537!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 538!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 539!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 540!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 543!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 547!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 548!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 549!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 550!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 551!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 555!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 557!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 558!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 559!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 560!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 561!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 562!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 563!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 564!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 565!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 566!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 567!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 568!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 569!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 571!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 572!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 574!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 575!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 577!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 578!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 582!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 584!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 585!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 587!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 589!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 590!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 593!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 594!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 595!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 596!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 598!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 601!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 602!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 603!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 604!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 605!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 607!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 608!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 609!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 610!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 611!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 618!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 619!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 620!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 621!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 622!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 623!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 624!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 625!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 626!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 627!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 631!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 632!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 633!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 634!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 635!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 636!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 637!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 641!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 649!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 650!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 653!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 655!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 661!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 665!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 667!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 669!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 673!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 682!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 685!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 693!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 694!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 695!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 697!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 698!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 703!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 704!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 710!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 720!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 723!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 726!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 727!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 734!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 739!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 745!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 747!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 748!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 749!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 756!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 759!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 762!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 765!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 766!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 767!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 771!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 773!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 779!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 781!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 783!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 784!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 785!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 790!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 793!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 795!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 800!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 802!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 804!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 806!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 807!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 808!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 809!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_religion 810!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [03:43<03:42, 223.00s/it]Loading checkpoint shards: 100%|██████████| 2/2 [04:18<00:00, 112.97s/it]Loading checkpoint shards: 100%|██████████| 2/2 [04:18<00:00, 129.48s/it]
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 2!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 4!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 5!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 10!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 11!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 12!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 14!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 17!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 18!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 20!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 21!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 22!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 23!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 24!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 25!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 26!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 28!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 29!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 30!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 34!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 35!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 36!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 37!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 41!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 44!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 47!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 50!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 51!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 52!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 53!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 55!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 56!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 58!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 59!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 60!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 61!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 62!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 63!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 65!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 66!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 68!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 69!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 70!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 71!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 73!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 74!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 75!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 79!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 83!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 86!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 87!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 91!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 95!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 98!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 100!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 101!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 103!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 104!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 105!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 107!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 108!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 109!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 110!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 111!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 112!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 113!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 114!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 135!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 136!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 137!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 138!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 140!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 141!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 143!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 144!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 145!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 147!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 148!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 150!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 152!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 154!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 157!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 158!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 164!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 165!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 166!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 171!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 172!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 174!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 178!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 179!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 185!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 188!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 193!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 194!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 196!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 198!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 199!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 201!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 202!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 203!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 206!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 207!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 209!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 210!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 219!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 220!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 223!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 224!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 229!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 233!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 249!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 250!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 251!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 252!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 255!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 258!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 261!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 262!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 263!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 269!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 270!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 277!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 278!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 279!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 281!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 284!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 294!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 299!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 304!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 306!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 324!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 328!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 332!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 339!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 340!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 341!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 342!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 348!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 355!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 356!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 357!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 359!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 360!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 362!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 373!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 374!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 382!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 383!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 384!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 385!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 388!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 389!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 391!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 392!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 393!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 394!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 395!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 396!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 397!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 398!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 399!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 407!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 408!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 409!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 410!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 411!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 413!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 415!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 416!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 418!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 419!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 420!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 421!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 422!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 423!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 424!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 426!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 427!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 428!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 429!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 430!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 431!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 432!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 433!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 434!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 435!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 436!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 437!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 438!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 439!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 440!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 441!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 442!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 448!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 451!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 452!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 454!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 455!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 456!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 458!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 463!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 468!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 469!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 470!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 471!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 475!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 476!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 478!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 479!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 480!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 481!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 485!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 488!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 490!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 493!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 495!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 497!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 499!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 500!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 502!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 503!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 504!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 505!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 508!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 509!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 510!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 511!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 519!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 520!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 521!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 522!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 523!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 526!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 527!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 528!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 531!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 532!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 533!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 534!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 535!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 536!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 537!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 538!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 539!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 540!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 543!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 547!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 548!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 549!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 550!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 551!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 555!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 557!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 558!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 559!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 560!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 561!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 562!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 563!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 564!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 565!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 566!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 567!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 568!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 569!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 571!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 572!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 574!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 575!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 577!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 578!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 582!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 584!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 585!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 587!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 589!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 590!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 593!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 594!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 595!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 596!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 598!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 601!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 602!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 603!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 604!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 605!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 607!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 608!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 609!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 610!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 611!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 618!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 619!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 620!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 621!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 622!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 623!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 624!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 625!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 626!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 627!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 631!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 632!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 633!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 634!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 635!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 636!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 637!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 641!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 649!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 650!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 653!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 655!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 661!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 665!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 667!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 669!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 673!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 682!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 685!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 693!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 694!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 695!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 697!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 698!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 703!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 704!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 710!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 720!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 723!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 726!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 727!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 734!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 739!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 745!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 747!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 748!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 749!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 756!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 759!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 762!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 765!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 766!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 767!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 771!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 773!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 779!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 781!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 783!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 784!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 785!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 790!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 793!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 795!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 800!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 802!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 804!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 806!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 807!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 808!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 809!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 810!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 812!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 813!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 814!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 815!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 816!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [01:57<01:57, 117.03s/it]Loading checkpoint shards: 100%|██████████| 2/2 [02:14<00:00, 58.20s/it] Loading checkpoint shards: 100%|██████████| 2/2 [02:14<00:00, 67.03s/it]
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 2!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 4!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 5!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 10!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 11!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 12!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 14!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 17!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 18!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 20!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 21!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 22!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 23!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 24!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 25!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 26!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 28!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 29!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 30!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 34!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 35!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 36!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 37!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 41!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 44!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 47!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 50!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 51!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 52!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 53!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 55!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 56!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 58!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 59!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 60!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 61!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 62!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 63!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 65!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 66!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 68!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 69!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 70!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 71!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 73!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 74!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 75!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 79!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 83!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 86!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 87!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 91!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 95!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 98!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 100!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 101!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 103!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 104!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 105!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 107!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 108!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 109!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 110!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 111!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 112!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 113!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 114!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 135!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 136!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 137!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 138!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 140!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 141!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 143!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 144!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 145!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 147!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 148!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 150!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 152!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 154!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 157!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 158!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 164!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 165!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 166!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 171!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 172!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 174!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 178!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 179!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 185!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 188!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 193!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 194!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 196!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 198!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 199!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 201!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 202!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 203!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 206!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 207!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 209!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 210!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 219!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 220!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 223!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 224!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 229!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 233!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 249!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 250!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 251!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 252!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 255!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 258!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 261!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 262!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 263!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 269!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 270!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 277!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 278!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 279!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 281!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 284!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 294!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 299!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 304!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 306!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 324!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 328!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 332!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 339!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 340!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 341!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 342!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 348!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 355!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 356!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 357!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 359!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 360!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 362!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 373!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 374!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 382!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 383!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 384!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 385!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 388!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 389!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 391!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 392!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 393!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 394!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 395!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 396!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 397!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 398!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 399!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 407!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 408!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 409!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 410!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 411!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 413!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 415!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 416!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 418!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 419!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 420!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 421!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 422!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 423!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 424!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 426!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 427!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 428!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 429!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 430!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 431!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 432!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 433!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 434!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 435!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 436!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 437!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 438!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 439!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 440!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 441!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 442!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 448!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 451!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 452!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 454!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 455!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 456!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 458!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 463!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 468!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 469!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 470!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 471!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 475!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 476!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 478!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 479!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 480!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 481!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 485!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 488!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 490!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 493!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 495!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 497!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 499!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 500!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 502!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 503!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 504!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 505!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 508!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 509!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 510!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 511!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 519!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 520!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 521!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 522!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 523!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 526!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 527!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 528!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 531!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 532!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 533!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 534!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 535!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 536!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 537!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 538!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 539!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 540!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 543!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 547!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 548!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 549!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 550!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 551!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 555!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 557!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 558!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 559!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 560!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 561!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 562!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 563!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 564!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 565!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 566!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 567!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 568!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 569!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 571!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 572!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 574!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 575!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 577!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 578!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 582!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 584!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 585!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 587!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 589!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 590!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 593!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 594!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 595!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 596!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 598!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 601!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 602!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 603!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 604!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 605!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 607!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 608!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 609!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 610!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 611!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 618!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 619!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 620!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 621!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 622!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 623!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 624!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 625!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 626!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 627!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 631!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 632!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 633!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 634!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 635!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 636!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 637!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 641!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 649!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 650!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 653!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 655!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 661!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 665!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 667!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 669!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 673!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 682!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 685!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 693!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 694!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 695!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 697!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 698!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 703!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 704!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 710!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 720!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 723!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 726!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 727!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 734!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 739!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 745!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 747!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 748!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 749!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 756!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 759!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 762!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 765!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 766!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 767!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 771!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 773!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 779!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 781!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 783!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 784!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 785!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 790!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 793!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 795!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 800!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 802!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 804!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 806!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 807!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 808!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 809!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 810!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 812!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 813!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 814!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 815!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_religion 816!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
