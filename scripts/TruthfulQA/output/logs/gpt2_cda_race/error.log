/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 2!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 4!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 5!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 11!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 12!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 17!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 18!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 20!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 21!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 22!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 24!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 25!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 26!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 28!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 29!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 30!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 34!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 37!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 50!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 51!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 53!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 56!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 58!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 60!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 65!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 66!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 68!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 69!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 71!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 73!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 74!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 75!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 79!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 83!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 87!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 95!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 100!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 101!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 103!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 104!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 105!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 107!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 109!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 111!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 113!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 114!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 135!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 140!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 145!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 148!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 158!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 164!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 165!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 166!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 171!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 172!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 178!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 194!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 196!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 198!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 199!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 201!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 202!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 203!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 207!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 209!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 220!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 223!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 249!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 250!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 251!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 255!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 261!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 263!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 269!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 270!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 278!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 284!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 299!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 304!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 306!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 332!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 339!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 342!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 355!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 356!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 357!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 359!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 360!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 362!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 383!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 384!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 385!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 388!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 389!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 391!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 392!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 393!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 394!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 395!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 396!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 397!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 398!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 399!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 407!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 410!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 415!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 418!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 419!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 420!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 421!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 422!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 423!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 424!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 426!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 427!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 428!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 429!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 430!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 431!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 432!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 433!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 434!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 435!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 436!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 437!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 438!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 439!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 440!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 441!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 442!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 451!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 458!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 463!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 468!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 475!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 478!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 488!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 490!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 493!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 495!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 497!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 499!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 502!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 503!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 504!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 505!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 508!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 509!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 510!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 511!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 519!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 520!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 521!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 522!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 527!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 528!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 531!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 532!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 533!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 534!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 535!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 536!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 539!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 540!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 543!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 548!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 549!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 551!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 558!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 559!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 560!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 561!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 562!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 563!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 565!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 566!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 567!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 568!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 577!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 578!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 582!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 585!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 589!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 590!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 593!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 594!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 595!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 596!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 601!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 603!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 604!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 605!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 607!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 608!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 609!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 610!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 611!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 618!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 619!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 620!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 621!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 622!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 623!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 624!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 625!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 626!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 631!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 632!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 633!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 635!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 637!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 641!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 649!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 653!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 667!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 669!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 693!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 694!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 695!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 697!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 703!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 704!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 720!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 723!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 727!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 734!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 739!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 745!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 747!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 748!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 749!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 759!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 771!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 779!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 783!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 784!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 785!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 793!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 794!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 804!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 806!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 807!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 808!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 809!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 815!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/u/busekorkmaz/.conda/envs/fms_at_work/lib/python3.10/site-packages/nltk/translate/bleu_score.py:552: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 2!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 4!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 5!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 11!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 12!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 17!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 18!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 20!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 21!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 22!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 24!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 25!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 26!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 28!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 29!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 30!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 34!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 37!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 50!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 51!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 53!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 56!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 58!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 60!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 65!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 66!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 68!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 69!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 71!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 73!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 74!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 75!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 79!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 83!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 87!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 95!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 100!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 101!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 103!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 104!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 105!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 107!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 109!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 111!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 113!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 114!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 135!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 140!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 145!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 148!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 158!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 164!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 165!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 166!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 171!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 172!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 178!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 194!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 196!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 198!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 199!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 201!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 202!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 203!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 207!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 209!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 220!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 223!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 249!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 250!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 251!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 255!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 261!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 263!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 269!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 270!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 278!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 284!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 299!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 304!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 306!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 332!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 339!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 342!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 355!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 356!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 357!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 359!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 360!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 362!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 383!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 384!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 385!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 388!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 389!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 391!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 392!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 393!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 394!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 395!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 396!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 397!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 398!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 399!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 407!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 410!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 415!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 418!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 419!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 420!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 421!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 422!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 423!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 424!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 426!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 427!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 428!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 429!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 430!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 431!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 432!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 433!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 434!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 435!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 436!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 437!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 438!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 439!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 440!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 441!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 442!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 451!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 458!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 463!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 468!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 475!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 478!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 488!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 490!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 493!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 495!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 497!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 499!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 502!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 503!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 504!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 505!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 508!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 509!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 510!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 511!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 519!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 520!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 521!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 522!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 527!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 528!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 531!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 532!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 533!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 534!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 535!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 536!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 539!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 540!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 543!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 548!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 549!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 551!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 558!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 559!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 560!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 561!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 562!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 563!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 565!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 566!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 567!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 568!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 577!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 578!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 582!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 585!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 589!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 590!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 593!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 594!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 595!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 596!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 601!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 603!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 604!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 605!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 607!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 608!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 609!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 610!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 611!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 618!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 619!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 620!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 621!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 622!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 623!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 624!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 625!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 626!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 631!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 632!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 633!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 635!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 637!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 641!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 649!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 653!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 667!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 669!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 693!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 694!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 695!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 697!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 703!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 704!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 720!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 723!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 727!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 734!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 739!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 745!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 747!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 748!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 749!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 759!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 771!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 779!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 783!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 784!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 785!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 793!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 794!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 804!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 806!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 807!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 808!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 809!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:140: UserWarning: Answers missing for gpt2_cda_race 815!
  questions = metrics.run_bleu_and_rouge(model_key, questions)
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|     | 1/2 [06:20<06:20, 380.77s/it]Loading checkpoint shards: 100%|| 2/2 [08:04<00:00, 218.04s/it]Loading checkpoint shards: 100%|| 2/2 [08:04<00:00, 242.45s/it]
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 2!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 4!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 5!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 11!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 12!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 17!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 18!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 20!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 21!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 22!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 24!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 25!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 26!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 28!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 29!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 30!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 34!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 37!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 50!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 51!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 53!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 56!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 58!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 60!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 65!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 66!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 68!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 69!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 71!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 73!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 74!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 75!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 79!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 83!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 87!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 95!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 100!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 101!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 103!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 104!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 105!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 107!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 109!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 111!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 113!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 114!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 135!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 140!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 145!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 148!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 158!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 164!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 165!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 166!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 171!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 172!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 178!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 194!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 196!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 198!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 199!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 201!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 202!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 203!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 207!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 209!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 220!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 223!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 249!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 250!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 251!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 255!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 261!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 263!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 269!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 270!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 278!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 284!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 299!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 304!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 306!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 332!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 339!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 342!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 355!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 356!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 357!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 359!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 360!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 362!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 383!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 384!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 385!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 388!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 389!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 391!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 392!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 393!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 394!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 395!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 396!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 397!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 398!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 399!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 407!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 410!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 415!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 418!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 419!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 420!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 421!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 422!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 423!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 424!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 426!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 427!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 428!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 429!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 430!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 431!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 432!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 433!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 434!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 435!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 436!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 437!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 438!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 439!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 440!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 441!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 442!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 451!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 458!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 463!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 468!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 475!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 478!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 488!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 490!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 493!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 495!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 497!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 499!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 502!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 503!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 504!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 505!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 508!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 509!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 510!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 511!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 519!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 520!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 521!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 522!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 527!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 528!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 531!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 532!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 533!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 534!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 535!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 536!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 539!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 540!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 543!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 548!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 549!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 551!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 558!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 559!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 560!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 561!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 562!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 563!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 565!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 566!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 567!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 568!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 577!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 578!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 582!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 585!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 589!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 590!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 593!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 594!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 595!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 596!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 601!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 603!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 604!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 605!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 607!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 608!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 609!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 610!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 611!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 618!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 619!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 620!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 621!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 622!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 623!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 624!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 625!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 626!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 631!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 632!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 633!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 635!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 637!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 641!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 649!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 653!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 667!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 669!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 693!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 694!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 695!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 697!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 703!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 704!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 720!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 723!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 727!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 734!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 739!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 745!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 747!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 748!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 749!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 759!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 771!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 779!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 783!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 784!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 785!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 793!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 794!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 804!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 806!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 807!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 808!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 809!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 815!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|     | 1/2 [00:27<00:27, 27.40s/it]Loading checkpoint shards: 100%|| 2/2 [00:29<00:00, 12.37s/it]Loading checkpoint shards: 100%|| 2/2 [00:29<00:00, 14.63s/it]
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 2!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 4!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 5!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 11!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 12!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 17!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 18!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 20!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 21!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 22!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 24!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 25!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 26!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 28!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 29!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 30!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 34!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 37!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 50!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 51!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 53!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 56!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 58!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 60!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 65!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 66!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 68!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 69!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 71!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 73!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 74!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 75!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 79!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 83!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 87!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 95!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 100!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 101!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 103!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 104!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 105!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 107!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 109!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 111!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 113!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 114!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 135!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 140!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 145!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 148!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 158!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 164!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 165!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 166!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 171!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 172!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 178!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 194!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 196!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 198!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 199!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 201!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 202!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 203!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 207!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 209!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 220!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 223!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 249!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 250!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 251!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 255!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 261!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 263!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 269!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 270!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 278!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 284!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 299!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 304!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 306!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 332!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 339!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 342!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 355!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 356!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 357!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 359!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 360!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 362!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 383!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 384!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 385!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 388!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 389!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 391!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 392!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 393!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 394!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 395!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 396!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 397!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 398!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 399!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 407!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 410!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 415!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 418!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 419!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 420!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 421!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 422!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 423!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 424!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 426!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 427!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 428!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 429!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 430!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 431!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 432!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 433!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 434!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 435!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 436!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 437!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 438!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 439!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 440!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 441!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 442!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 451!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 458!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 463!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 468!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 475!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 478!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 488!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 490!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 493!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 495!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 497!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 499!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 502!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 503!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 504!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 505!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 508!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 509!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 510!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 511!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 519!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 520!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 521!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 522!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 527!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 528!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 531!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 532!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 533!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 534!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 535!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 536!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 539!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 540!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 543!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 548!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 549!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 551!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 558!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 559!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 560!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 561!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 562!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 563!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 565!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 566!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 567!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 568!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 577!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 578!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 582!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 585!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 589!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 590!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 593!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 594!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 595!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 596!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 601!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 603!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 604!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 605!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 607!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 608!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 609!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 610!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 611!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 618!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 619!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 620!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 621!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 622!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 623!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 624!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 625!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 626!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 631!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 632!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 633!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 635!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 637!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 641!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 649!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 653!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 667!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 669!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 693!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 694!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 695!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 697!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 703!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 704!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 720!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 723!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 727!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 734!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 739!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 745!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 747!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 748!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 749!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 759!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 771!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 779!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 783!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 784!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 785!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 793!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 794!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 804!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 806!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 807!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 808!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 809!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
/dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/TruthfulQA/truthfulqa/evaluate.py:158: UserWarning: Answer missing for gpt2_cda_race 815!
  questions = metrics.run_end2end_llama(model_key, metric, questions, device=torch_device, cache_dir=args.cache_dir)
r)
