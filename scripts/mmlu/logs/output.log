
Evaluating model: instructive_debiasing
Average accuracy 0.240 - high_school_computer_science
Average accuracy 0.170 - college_chemistry
Average accuracy 0.229 - human_sexuality
Average accuracy 0.306 - jurisprudence
Average accuracy 0.209 - miscellaneous
Average accuracy 0.280 - moral_disputes
Average accuracy 0.244 - professional_law
Average accuracy 0.261 - high_school_microeconomics
Average accuracy 0.230 - anatomy
Average accuracy 0.223 - high_school_government_and_politics
Average accuracy 0.233 - management
Average accuracy 0.273 - public_relations
Average accuracy 0.222 - high_school_psychology
Average accuracy 0.229 - high_school_biology
Average accuracy 0.278 - security_studies
Average accuracy 0.225 - prehistory
Average accuracy 0.140 - international_law
Average accuracy 0.190 - medical_genetics
Average accuracy 0.281 - econometrics
Average accuracy 0.230 - conceptual_physics
Average accuracy 0.285 - high_school_physics
Average accuracy 0.210 - college_mathematics
Average accuracy 0.241 - high_school_world_history
Average accuracy 0.211 - virology
Average accuracy 0.330 - global_facts
Average accuracy 0.292 - high_school_statistics
Average accuracy 0.215 - clinical_knowledge
Average accuracy 0.300 - college_computer_science
Average accuracy 0.244 - high_school_mathematics
Average accuracy 0.207 - electrical_engineering
Average accuracy 0.301 - logical_fallacies
Average accuracy 0.261 - moral_scenarios
Average accuracy 0.290 - professional_medicine
Average accuracy 0.286 - formal_logic
Average accuracy 0.229 - nutrition
Average accuracy 0.279 - sociology
Average accuracy 0.206 - college_physics
Average accuracy 0.263 - high_school_geography
Average accuracy 0.233 - high_school_macroeconomics
Average accuracy 0.214 - marketing
Average accuracy 0.280 - us_foreign_policy
Average accuracy 0.230 - high_school_us_history
Average accuracy 0.238 - philosophy
Average accuracy 0.315 - high_school_chemistry
Average accuracy 0.296 - astronomy
Average accuracy 0.295 - machine_learning
Average accuracy 0.266 - professional_accounting
Average accuracy 0.200 - abstract_algebra
Average accuracy 0.270 - elementary_mathematics
Average accuracy 0.211 - world_religions
Average accuracy 0.255 - professional_psychology
Average accuracy 0.285 - college_biology
Average accuracy 0.215 - human_aging
Average accuracy 0.265 - college_medicine
Average accuracy 0.220 - business_ethics
Average accuracy 0.220 - computer_security
Average accuracy 0.218 - high_school_european_history
Average accuracy for instructive_debiasing: 0.246
Detailed results for instructive_debiasing logged to: /dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/mmlu/output/instructive_debiasing_results_20240902_103341.jsonl

Evaluating model: gpt2
Average accuracy 0.240 - high_school_computer_science
Average accuracy 0.160 - college_chemistry
Average accuracy 0.260 - human_sexuality
Average accuracy 0.306 - jurisprudence
Average accuracy 0.211 - miscellaneous
Average accuracy 0.280 - moral_disputes
Average accuracy 0.245 - professional_law
Average accuracy 0.273 - high_school_microeconomics
Average accuracy 0.207 - anatomy
Average accuracy 0.197 - high_school_government_and_politics
Average accuracy 0.233 - management
Average accuracy 0.291 - public_relations
Average accuracy 0.233 - high_school_psychology
Average accuracy 0.252 - high_school_biology
Average accuracy 0.269 - security_studies
Average accuracy 0.228 - prehistory
Average accuracy 0.140 - international_law
Average accuracy 0.220 - medical_genetics
Average accuracy 0.263 - econometrics
Average accuracy 0.234 - conceptual_physics
Average accuracy 0.245 - high_school_physics
Average accuracy 0.210 - college_mathematics
Average accuracy 0.228 - high_school_world_history
Average accuracy 0.211 - virology
Average accuracy 0.370 - global_facts
Average accuracy 0.278 - high_school_statistics
Average accuracy 0.215 - clinical_knowledge
Average accuracy 0.310 - college_computer_science
Average accuracy 0.244 - high_school_mathematics
Average accuracy 0.214 - electrical_engineering
Average accuracy 0.276 - logical_fallacies
Average accuracy 0.268 - moral_scenarios
Average accuracy 0.279 - professional_medicine
Average accuracy 0.278 - formal_logic
Average accuracy 0.219 - nutrition
Average accuracy 0.264 - sociology
Average accuracy 0.196 - college_physics
Average accuracy 0.293 - high_school_geography
Average accuracy 0.215 - high_school_macroeconomics
Average accuracy 0.209 - marketing
Average accuracy 0.260 - us_foreign_policy
Average accuracy 0.230 - high_school_us_history
Average accuracy 0.228 - philosophy
Average accuracy 0.296 - high_school_chemistry
Average accuracy 0.316 - astronomy
Average accuracy 0.295 - machine_learning
Average accuracy 0.252 - professional_accounting
Average accuracy 0.170 - abstract_algebra
Average accuracy 0.262 - elementary_mathematics
Average accuracy 0.228 - world_religions
Average accuracy 0.239 - professional_psychology
Average accuracy 0.292 - college_biology
Average accuracy 0.291 - human_aging
Average accuracy 0.249 - college_medicine
Average accuracy 0.270 - business_ethics
Average accuracy 0.210 - computer_security
Average accuracy 0.218 - high_school_european_history
Average accuracy for gpt2: 0.246
Detailed results for gpt2 logged to: /dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/mmlu/output/gpt2_results_20240902_103652.jsonl

Evaluating model: sentence_debiasing
Average accuracy 0.250 - high_school_computer_science
Average accuracy 0.180 - college_chemistry
Average accuracy 0.260 - human_sexuality
Average accuracy 0.241 - jurisprudence
Average accuracy 0.246 - miscellaneous
Average accuracy 0.205 - moral_disputes
Average accuracy 0.255 - professional_law
Average accuracy 0.294 - high_school_microeconomics
Average accuracy 0.230 - anatomy
Average accuracy 0.176 - high_school_government_and_politics
Average accuracy 0.204 - management
Average accuracy 0.227 - public_relations
Average accuracy 0.211 - high_school_psychology
Average accuracy 0.223 - high_school_biology
Average accuracy 0.347 - security_studies
Average accuracy 0.256 - prehistory
Average accuracy 0.273 - international_law
Average accuracy 0.230 - medical_genetics
Average accuracy 0.202 - econometrics
Average accuracy 0.209 - conceptual_physics
Average accuracy 0.238 - high_school_physics
Average accuracy 0.200 - college_mathematics
Average accuracy 0.228 - high_school_world_history
Average accuracy 0.247 - virology
Average accuracy 0.340 - global_facts
Average accuracy 0.199 - high_school_statistics
Average accuracy 0.226 - clinical_knowledge
Average accuracy 0.270 - college_computer_science
Average accuracy 0.237 - high_school_mathematics
Average accuracy 0.241 - electrical_engineering
Average accuracy 0.227 - logical_fallacies
Average accuracy 0.244 - moral_scenarios
Average accuracy 0.224 - professional_medicine
Average accuracy 0.310 - formal_logic
Average accuracy 0.216 - nutrition
Average accuracy 0.249 - sociology
Average accuracy 0.206 - college_physics
Average accuracy 0.273 - high_school_geography
Average accuracy 0.282 - high_school_macroeconomics
Average accuracy 0.171 - marketing
Average accuracy 0.240 - us_foreign_policy
Average accuracy 0.235 - high_school_us_history
Average accuracy 0.209 - philosophy
Average accuracy 0.266 - high_school_chemistry
Average accuracy 0.303 - astronomy
Average accuracy 0.330 - machine_learning
Average accuracy 0.234 - professional_accounting
Average accuracy 0.230 - abstract_algebra
Average accuracy 0.265 - elementary_mathematics
Average accuracy 0.222 - world_religions
Average accuracy 0.211 - professional_psychology
Average accuracy 0.333 - college_biology
Average accuracy 0.359 - human_aging
Average accuracy 0.266 - college_medicine
Average accuracy 0.220 - business_ethics
Average accuracy 0.200 - computer_security
Average accuracy 0.212 - high_school_european_history
Average accuracy for sentence_debiasing: 0.243
Detailed results for sentence_debiasing logged to: /dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/mmlu/output/sentence_debiasing_results_20240902_103943.jsonl

Evaluating model: inlp
Average accuracy 0.250 - high_school_computer_science
Average accuracy 0.200 - college_chemistry
Average accuracy 0.252 - human_sexuality
Average accuracy 0.250 - jurisprudence
Average accuracy 0.259 - miscellaneous
Average accuracy 0.272 - moral_disputes
Average accuracy 0.263 - professional_law
Average accuracy 0.332 - high_school_microeconomics
Average accuracy 0.304 - anatomy
Average accuracy 0.238 - high_school_government_and_politics
Average accuracy 0.243 - management
Average accuracy 0.209 - public_relations
Average accuracy 0.250 - high_school_psychology
Average accuracy 0.210 - high_school_biology
Average accuracy 0.351 - security_studies
Average accuracy 0.293 - prehistory
Average accuracy 0.289 - international_law
Average accuracy 0.220 - medical_genetics
Average accuracy 0.281 - econometrics
Average accuracy 0.260 - conceptual_physics
Average accuracy 0.225 - high_school_physics
Average accuracy 0.210 - college_mathematics
Average accuracy 0.232 - high_school_world_history
Average accuracy 0.193 - virology
Average accuracy 0.300 - global_facts
Average accuracy 0.208 - high_school_statistics
Average accuracy 0.238 - clinical_knowledge
Average accuracy 0.300 - college_computer_science
Average accuracy 0.233 - high_school_mathematics
Average accuracy 0.241 - electrical_engineering
Average accuracy 0.282 - logical_fallacies
Average accuracy 0.260 - moral_scenarios
Average accuracy 0.254 - professional_medicine
Average accuracy 0.286 - formal_logic
Average accuracy 0.222 - nutrition
Average accuracy 0.284 - sociology
Average accuracy 0.216 - college_physics
Average accuracy 0.303 - high_school_geography
Average accuracy 0.277 - high_school_macroeconomics
Average accuracy 0.235 - marketing
Average accuracy 0.210 - us_foreign_policy
Average accuracy 0.314 - high_school_us_history
Average accuracy 0.222 - philosophy
Average accuracy 0.305 - high_school_chemistry
Average accuracy 0.276 - astronomy
Average accuracy 0.321 - machine_learning
Average accuracy 0.252 - professional_accounting
Average accuracy 0.220 - abstract_algebra
Average accuracy 0.230 - elementary_mathematics
Average accuracy 0.193 - world_religions
Average accuracy 0.225 - professional_psychology
Average accuracy 0.264 - college_biology
Average accuracy 0.345 - human_aging
Average accuracy 0.254 - college_medicine
Average accuracy 0.240 - business_ethics
Average accuracy 0.220 - computer_security
Average accuracy 0.212 - high_school_european_history
Average accuracy for inlp: 0.256
Detailed results for inlp logged to: /dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/mmlu/output/inlp_results_20240902_104241.jsonl
Results summary written to: /dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/mmlu/output/results_summary.csv

------------------------------------------------------------
Sender: LSF System <lsfadmin@cccxc421.pok.ibm.com>
Subject: Job 4834821: <python /dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/mmlu/evaluate.py --models instructive_debiasing gpt2 sentence_debiasing inlp --save_dir /dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/mmlu/output --ntrain 1> in cluster <cccCluster> Done

Job <python /dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/mmlu/evaluate.py --models instructive_debiasing gpt2 sentence_debiasing inlp --save_dir /dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/mmlu/output --ntrain 1> was submitted from host <cccxl012> by user <busekorkmaz> in cluster <cccCluster> at Mon Sep  2 10:33:08 2024
Job was executed on host(s) <4*cccxc421>, in queue <x86_6h>, as user <busekorkmaz> in cluster <cccCluster> at Mon Sep  2 10:33:10 2024
</u/busekorkmaz> was used as the home directory.
</dccstor/autofair/busekorkmaz/factual-bias-mitigation> was used as the working directory.
Started at Mon Sep  2 10:33:10 2024
Terminated at Mon Sep  2 10:45:39 2024
Results reported at Mon Sep  2 10:45:39 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python /dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/mmlu/evaluate.py --models instructive_debiasing gpt2 sentence_debiasing inlp --save_dir /dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/mmlu/output --ntrain 1
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   717.18 sec.
    Max Memory :                                 1127 MB
    Average Memory :                             727.95 MB
    Total Requested Memory :                     45056.00 MB
    Delta Memory :                               43929.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                18
    Run time :                                   749 sec.
    Turnaround time :                            751 sec.

The output (if any) is above this job summary.



PS:

Read file </dccstor/autofair/busekorkmaz/factual-bias-mitigation/scripts/mmlu/logs//error.log> for stderr output of this job.

